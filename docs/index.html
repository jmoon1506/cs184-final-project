<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 184 Mesh Editor</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

</head>

<body>
<h1 align="middle">CS 184: Final Project, Spring 2018</h1>
<h1 align="middle">Real-time 2D Global Illumination</h1>
<h2 align="middle">Joseph Moon, Abdul AlZanki, Lawrence Elkins</h2>
<br/><br/>
<div>

<div align="left">
  <div align="middle">
    <h2>Title, Summary and Team Members</h2>
  </div>
		<p>For this project, we hope to implement an interactive 2D raytracer. We will start out by implementing a Monte Carlo raytracer similar to the one in Assignment 3 - with the ability to run in real-time. However, it would run at low-fidelity during scene changes. Our more ambitious goal is to create a parallel raytracer, which would have the ability to render billions of samples per second on consumer hardware and enable high-fidelity dynamic scenes.
			<br/><br/>
			Joseph Moon: 3033156362
			<br/><br/>
			Abdulrahman AlZenki: 25740621
			<br/><br/>
			Lawrence Elkins: 23452545
			<br/><br/>
		</p>

    <div align="middle">
      <h2>Problem Description</h2>
    </div>

		<p>Raytracing is a powerful method to generate realistic lighting effects, such as global illumination and soft shadows. It works by shooting rays into each screen pixel from the camera and approximating a solution to the rendering equation along each ray's intersections with the scene. An ongoing focus of research is raytracing in real-time, due to the algorithm's steep computational cost. We plan to mitigate this in several ways:</p>
		<ul>
			<li>Run the renderer in 2D space instead of 3D space to decrease complexity.</li>
			<li>When the camera or scene changes, we need to re-render the entire scene. To maintain real-time performance, we might render frames with just a few samples and gradually add new ones if the scene and camera stay still (i.e. lo-fi dynamic scenes).</li>
			<li>More ambitiously, we want to run raytracing on the GPU with a parallel ray-bundling algorithm to create hi-fi dynamic scenes.</li>
		</ul>

		<br/><br/>

    <div align="middle">
      <h2>Goals and Deliverables</h2>
    </div>
		<ul>
			<li>TODO: from spec: "Define how you will measure the quality / performance of your system (e.g. graphs showing speedup, or quantifying accuracy). It may not be possible to define precise target metrics at this time, but we encourage you to try."</li>
			<li>TODO: from spec: "What questions do you plan to answer with your analysis?"</li>
    </ul>

    <div align="left">
      <h3>What we plan to deliver: Real-time 2D Monte Carlo Path Tracing</h3>
    </div>
			<ul>
				<li>Render simple 2D scenes in real-time (limited number of lights and objects).</li>
				<li>To show that our renderer is in real-time, we will need to have the scene either be animating/changing in real-time or have the ability to interact with it.</li>
        <li>We will initially try the radiosity equation from class. If that doesn't work, then we will just deposit light along rays and adjust gamma (like in <a href="https://benedikt-bitterli.me/tantalum/tantalum.html">this resource</a>).</li>
				<li>In this implementation, we will sort of "lazy load" (TODO: Is this a good idea to refer to it as lazy loading, or do you think we shouldn't?) the scene after movements. This means that the quality of the real-time render might start out a bit noisy when things are first moved. For example: </li>

				<br/><br/>

				<div align="middle">
				  <table style="width=100%">
				    <tr>
				      <td>
				        <img src="images/pathtracer_low_samples.png" align="middle" width="300px"/>
				        <figcaption align="middle">How it looks when changing the scene</figcaption>
				      </td>
				      <td>
				        <img src="images/pathtracer_high_samples.png" align="middle" width="300px"/>
				        <figcaption align="middle">After staying still for several seconds</figcaption>
				      </td>
				    </tr>
				  </table>
				</div>
				<br/><br/>
        <li>Downsides</li>
        <ul>
          <li>Adjusting the camera or scene requires resampling the entire image, which is costly.</li>
          <li>Runs on CPU, or inefficiently on GPU</li>
          <li>Not truly real-time because of the quality of the rendering after movement. Impractical for actual applications</li>
          <li>Line deposition not physically-based, doesn't looks realistic unless we have a ridiculous number of samples</li>
        </ul>
        <li>Upsides</li>
        <ul>
          <li>Easier to render caustics and specular surfaces</li>
        </ul>
			</ul>
      <div align="left">
        <h3>What we hope to deliver: Real-time 2D Global Line Radiosity</h3>
      </div>
      <div align="middle">
        <table style="width=100%">
          <tr>
            <td>
              <img src="images/global_line_radiosity1.png" align="middle" width="300px"/>
              <figcaption align="middle">2D Global Line Radiosity</figcaption>
            </td>
          </tr>
        </table>
      </div>
      <br/><br/>
      <div align="middle">
        <table style="width=100%">
          <tr>
            <td>
              <img src="images/global_line_radiosity_direct.png" align="middle" width="300px"/>
              <figcaption align="middle">Only direct lighting</figcaption>
            </td>
            <td>
              <img src="images/global_line_radiosity_combined.png" align="middle" width="300px"/>
              <figcaption align="middle">Add indirect lighting</figcaption>
            </td>
          </tr>
        </table>
      </div>
      <br/><br/>
			<ul>
				<li>Parallel Ray-Bundling</li>
        <ul>
          <li>Each sample calculates radiosity equation at every pixel, but with rays all in the same direction.</li>
          <li>Sampling many times, with many solid angles, yields the equivalent result to raytracing.</li>
          <li>Essentially rasterizes with each sample, making better use of GPU than raytracing.</li>
        </ul>
        <br/><br/>
        <div align="middle">
          <table style="width=100%">
            <tr>
              <td>
                <img src="images/parallel_ray_bundling.png" align="middle" width="300px"/>
                <figcaption align="middle">Instead of raycasting randomly from different positions, we use bundles of parallel rays. Both methods yield the same result (Hachisuka 2005).</figcaption>
              </td>
              <td>
                <img src="images/parallel_ray_bundling2.png" align="middle" width="300px"/>
                <figcaption align="middle">Radiance is passed between intersection points.</figcaption>
              </td>
              <td>
                <img src="images/parallel_ray_bounce.png" align="middle" width="300px"/>
                <figcaption align="middle">Similar to traditional pathtracing, we can calculate bounces recursively.</figcaption>
              </td>
            </tr>
          </table>
        </div>
        <br/><br/>
        <li>If possible, we might use raymarching and distance functions instead of primitives and intersection tests.</li>
        <li>If possible, we might also use Cheap soft shadows, sub-surface scattering, ambient occlusion.</li>
        <br/><br/>
        <div align="middle">
          <table style="width=100%">
            <tr>
              <td>
                <img src="images/global_line_radiosity_sss.png" align="middle" width="600px"/>
                <figcaption align="middle">We can cheaply calculate lighting effects, such as sub-surface scattering</figcaption>
              </td>
            </tr>
          </table>
        </div>
        <br/><br/>
        <li>We think it will be possible to accomplish real-time global illumination because we have examples of people who have done so in our refrences. (TODO: this is not a good answer, expand on it).</li>
			</ul>
		<br/><br/>
    <div align="middle">
      <h2>Schedule</h2>
    </div>
    <p>Week 1 (April 2 - 8):</p>
      <ul>
        <li>Set up framework to work on project.</li>
        <li>Begin attempting to implement Global Line Radiosity.</li>
      </ul>  
    <p>Week 2 (April 9 - 15):</p>
      <ul>
        <li>If Global Line Radiosity is feasible, try to finish static implementation and begin working on animating simple scenes.  Also investigate 
        methods of speeding up the algorithms and making the renders more impressive with details like simple particle effects.</li>
        <li>Otherwise, begin working on real time Monte Carlo pathtracer and investigate methods of speeding up the algorithm or approximating parts of it (to deal with the inevitable noise that will be produced).</li>
      </ul>
    <p>Week 3 (April 16 - 22):</p>
      <ul>
        <li>Finalize (hopefully) both implementations of real time 2D global illumination so that speed and image quality can be compared between
        the two methods.</li>
        <li>Write milestone report and slides.  Also create milestone video that will include several simple scenes to display the methods in action.</li>
    <p>Week 4 (April 23 - 29):</p>
      <ul>
        <li>Polish the project, adding in any final speed-ups or details that can enhance image quality.</li>
        <li>Work on presentation slides and script.  Brainstorm ideas to extend the project or potential issues that would arise in certain configurations</li>
        <li>Develop several slightly more complicated scenes that can display the power (and possibly the limitations) of our implementation.</li>
        <li>Begin work on final report, to be finalized early in the following week.</li>
      </ul>

<br/><br/>

    <div align="middle">
      <h2>Resources</h2>
    </div>
		<ul>
      <li>J. Hermes, N. Henrich, T. Grosch, and S. Mueller, "Global Illumination using Parallel Global Ray-Bundles," <i>Vision, Modeling, and Visualization</i>, 2010.</li>
      <li>J. Mortensen, P. Khanna, and M. Slater, "Light Field Propagation and Rendering on the GPU," The Association for Computing Machinery, 2007.</li>
      <li>T. Hachisuka, "High-Quality Global Illumination Rendering Using Rasterization," in <i>GPU Gems 2</i>, Matt Pharr, Ed., 2005.</li>
      <li>Y. Tokuyoshi, T. Sekine, and S. Ogaki, "Fast Global Illumination Baking via Ray-Bundles," THe Association for Computing Machinery, 2011.</li>
      <li>A. Thomsen and K. H. Nielsen, "Approximate Radiosity Using Stochastic Depth Buffering," Journal of Graphics, GPU, and Game Tools, 2011.</li>
      <li>The blog of Thomas Diewald</li>
      <li>The blog of Benedikt Bitterli</li>
      <li>WebGL and the THREE.js library</li>
      <li>Additional libraries for any potential accessory features like particle simulation libraries, etc.</li>
		</ul>
</div>

<!-- TODO:
I. 2D simplifications
  a. Only need to sample pixels on the 2D plane
  b. Only need to cast rays in 360 degrees, instead of an entire sphere
IV. Add related works
  > Monte Carlo pathtracer example: https://benedikt-bitterli.me/tantalum/tantalum.html
  > 2D Global Line Radiosity (currently down): http://thomasdiewald.com/blog/?p=2949
  > Papers (see references directory)
-->

</body>
</html>
